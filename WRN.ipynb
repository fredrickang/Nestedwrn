{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%run hyperparameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BN_EPSILON = 0.00001\n",
    "BN_DECAY = 0.999\n",
    "\n",
    "## channel scheduling factors (now, 3 levels for conv scheduling)\n",
    "l2r = 1.0/2.0    # actual density: (l2r^2) *100\n",
    "l1r = 1.0/4.0    # actual density: (l1r^2) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variables(name, shape):\n",
    "\n",
    "    n1 = np.sqrt(6. / (shape[0] * shape[1] * l1r * (shape[-2] + shape[-1])))\n",
    "    n2 = np.sqrt(6. / (shape[0] * shape[1] * l2r * (shape[-2] + shape[-1])))\n",
    "    n3 = np.sqrt(6. / (shape[0] * shape[1] * (shape[-2] + shape[-1])))\n",
    "\n",
    "    shape1 = [shape[0], shape[1], int(l1r * shape[2]), int(l1r * shape[3])]\n",
    "    shape2_1 = [shape[0], shape[1], int((l2r - l1r) * shape[2]), int(l1r * shape[3])]\n",
    "    shape2_2 = [shape[0], shape[1], int(l2r * shape[2]), int((l2r - l1r) * shape[3])]\n",
    "    shape3_1 = [shape[0], shape[1], int((1. - l2r) * shape[2]), int(l2r * shape[3])]\n",
    "    shape3_2 = [shape[0], shape[1], shape[2], int((1. - l2r) * shape[3])]\n",
    "\n",
    "    lv1_variables = tf.get_variable(name + '_l1', initializer=tf.random_uniform(shape1, -n3, n3, tf.float32, seed=None))\n",
    "    lv2_1_variables = tf.get_variable(name + '_l2_1', initializer=tf.random_uniform(shape2_1, -n3, n3, tf.float32, seed=None))\n",
    "    lv2_2_variables = tf.get_variable(name + '_l2_2', initializer=tf.random_uniform(shape2_2, -n3, n3, tf.float32, seed=None))\n",
    "    lv3_1_variables = tf.get_variable(name + '_l3_1', initializer=tf.random_uniform(shape3_1, -n3, n3, tf.float32, seed=None))\n",
    "    lv3_2_variables = tf.get_variable(name + '_l3_2', initializer=tf.random_uniform(shape3_2, -n3, n3, tf.float32, seed=None))\n",
    "\n",
    "    return lv1_variables, lv2_1_variables, lv2_2_variables, lv3_1_variables, lv3_2_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer(input1, input2, input3, num_labels):\n",
    "\n",
    "    input_dim1 = input1.get_shape().as_list()[-1]\n",
    "    input_dim2 = input2.get_shape().as_list()[-1]\n",
    "    input_dim3 = input3.get_shape().as_list()[-1]\n",
    "\n",
    "    fc_w1 = tf.get_variable('fc_weights_l1', shape=[input_dim1, num_labels], initializer=tf.initializers.variance_scaling(scale=1.0))\n",
    "    fc_w2 = tf.get_variable('fc_weights_l2', shape=[input_dim2, num_labels], initializer=tf.initializers.variance_scaling(scale=1.0))\n",
    "    fc_w3 = tf.get_variable('fc_weights_l3', shape=[input_dim3, num_labels], initializer=tf.initializers.variance_scaling(scale=1.0))\n",
    "\n",
    "    fc_b1 = tf.get_variable(name='fc_bias_l1', shape=[num_labels], initializer=tf.zeros_initializer())\n",
    "    fc_b2 = tf.get_variable(name='fc_bias_l2', shape=[num_labels], initializer=tf.zeros_initializer())\n",
    "    fc_b3 = tf.get_variable(name='fc_bias_l3', shape=[num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    fc_h1 = tf.matmul(input1, fc_w1) + fc_b1\n",
    "    fc_h2 = tf.matmul(input2, fc_w2) + fc_b2\n",
    "    fc_h3 = tf.matmul(input3, fc_w3) + fc_b3\n",
    "    return fc_h1, fc_h2, fc_h3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization_layer(name, input_layer, dimension, is_training=True):\n",
    "\n",
    "    beta = tf.get_variable(name + 'beta', dimension, tf.float32, initializer=tf.constant_initializer(0.0, tf.float32))\n",
    "    gamma = tf.get_variable(name + 'gamma', dimension, tf.float32, initializer=tf.constant_initializer(1.0, tf.float32))\n",
    "    mu = tf.get_variable(name + 'mu', dimension, tf.float32, initializer=tf.constant_initializer(0.0, tf.float32), trainable=False)\n",
    "    sigma = tf.get_variable(name + 'sigma', dimension, tf.float32, initializer=tf.constant_initializer(1.0, tf.float32), trainable=False)\n",
    "\n",
    "    if is_training is True:\n",
    "        mean, variance = tf.nn.moments(input_layer, axes=[0, 1, 2])\n",
    "        train_mean = tf.assign(mu, mu * BN_DECAY + mean * (1 - BN_DECAY))\n",
    "        train_var = tf.assign(sigma, sigma * BN_DECAY + variance * (1 - BN_DECAY))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(input_layer, mean, variance, beta, gamma, BN_EPSILON)\n",
    "    else:\n",
    "        bn_layer = tf.nn.batch_normalization(input_layer, mu, sigma, beta, gamma, BN_EPSILON)\n",
    "\n",
    "    return bn_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input_layer, filter_shape, stride, is_training):\n",
    "\n",
    "    in_channel = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    bn_layer = batch_normalization_layer('l1_l2_l3', input_layer, in_channel, is_training)\n",
    "    bn_layer = tf.nn.relu(bn_layer)\n",
    "\n",
    "    n1 = np.sqrt(6. / (filter_shape[0] * filter_shape[1] * (filter_shape[-2] + l1r * filter_shape[-1])))\n",
    "    n2 = np.sqrt(6. / (filter_shape[0] * filter_shape[1] * (filter_shape[-2] + l2r * filter_shape[-1])))\n",
    "    n3 = np.sqrt(6. / (filter_shape[0] * filter_shape[1] * (filter_shape[-2] + filter_shape[-1])))\n",
    "    filter1 = tf.get_variable('conv_l1',\n",
    "                              initializer=tf.random_uniform([filter_shape[0], filter_shape[1], filter_shape[2], int(l1r*filter_shape[3])],\n",
    "                                                            -n3, n3,   tf.float32, seed=None))\n",
    "    filter2 = tf.get_variable('conv_l2',\n",
    "                              initializer=tf.random_uniform([filter_shape[0], filter_shape[1], filter_shape[2], int((l2r-l1r)*filter_shape[3])],\n",
    "                                                            -n3, n3,   tf.float32, seed=None))\n",
    "    filter3 = tf.get_variable('conv_l3',\n",
    "                              initializer=tf.random_uniform([filter_shape[0], filter_shape[1], filter_shape[2], int((1.-l2r)*filter_shape[3])],\n",
    "                                                            -n3, n3,   tf.float32, seed=None))\n",
    "\n",
    "    conv1 = tf.nn.conv2d(bn_layer, filter1, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    conv2 = tf.concat((conv1, tf.nn.conv2d(bn_layer, filter2, strides=[1, stride, stride, 1], padding='SAME')), 3)\n",
    "    conv3 = tf.concat((conv2, tf.nn.conv2d(bn_layer, filter3, strides=[1, stride, stride, 1], padding='SAME')), 3)\n",
    "\n",
    "    return conv1, conv2, conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_relu_conv_layer(input1, input2, input3, filter_shape, stride, is_training):\n",
    "\n",
    "    in_channel1 = input1.get_shape().as_list()[-1]\n",
    "    in_channel2 = input2.get_shape().as_list()[-1]\n",
    "    in_channel3 = input3.get_shape().as_list()[-1]\n",
    "\n",
    "    bn_layer1 = batch_normalization_layer('l1', input1, in_channel1, is_training)\n",
    "    bn_layer1 = tf.nn.relu(bn_layer1)\n",
    "    bn_layer2 = batch_normalization_layer('l2', input2, in_channel2, is_training)\n",
    "    bn_layer2 = tf.nn.relu(bn_layer2)\n",
    "    bn_layer3 = batch_normalization_layer('l3', input3, in_channel3, is_training)\n",
    "    bn_layer3 = tf.nn.relu(bn_layer3)\n",
    "\n",
    "    filter1, filter2_1, filter2_2, filter3_1, filter3_2 = create_variables(name='conv', shape=filter_shape)\n",
    "\n",
    "    conv1 = tf.nn.conv2d(bn_layer1, filter1, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    conv2 = tf.concat((tf.add(tf.nn.conv2d(bn_layer2[:, :, :, :int(l1r * filter_shape[2])], filter1, strides=[1, stride, stride, 1], padding='SAME'),\n",
    "                              tf.nn.conv2d(bn_layer2[:, :, :, int(l1r * filter_shape[2]):int(l2r * filter_shape[2])], filter2_1, strides=[1, stride, stride, 1],\n",
    "                                           padding='SAME')),\n",
    "                       tf.nn.conv2d(bn_layer2, filter2_2, strides=[1, stride, stride, 1], padding='SAME')), 3)\n",
    "    conv3 = tf.concat((tf.add(tf.nn.conv2d(bn_layer3[:, :, :, :int(l1r * filter_shape[2])], filter1, strides=[1, stride, stride, 1], padding='SAME'),\n",
    "                              tf.nn.conv2d(bn_layer3[:, :, :, int(l1r * filter_shape[2]):int(l2r * filter_shape[2])], filter2_1, strides=[1, stride, stride, 1],\n",
    "                                           padding='SAME')),\n",
    "                       tf.nn.conv2d(bn_layer3[:, :, :, :int(l2r * filter_shape[2])], filter2_2, strides=[1, stride, stride, 1], padding='SAME')), 3)\n",
    "    conv3 = tf.concat((tf.add(conv3, tf.nn.conv2d(bn_layer3[:, :, :, int(l2r * filter_shape[2]):], filter3_1, strides=[1, stride, stride, 1], padding='SAME')),\n",
    "                       tf.nn.conv2d(bn_layer3, filter3_2, strides=[1, stride, stride, 1], padding='SAME')), 3)\n",
    "\n",
    "    return conv1, conv2, conv3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(input1, input2, input3, output_channel, wide_scale, is_training, first_block=False):\n",
    "\n",
    "    input_channel = input3.get_shape().as_list()[-1]\n",
    "\n",
    "    # When it's time to \"shrink\" the image size, we use stride = 2\n",
    "    output_channel = int(output_channel * wide_scale)\n",
    "\n",
    "    if input_channel * wide_scale == output_channel:\n",
    "        increase_dim = True\n",
    "        stride = 1\n",
    "    else:\n",
    "        if input_channel * 2 == output_channel:\n",
    "            increase_dim = True\n",
    "            stride = 2\n",
    "        elif input_channel == output_channel:\n",
    "            increase_dim = False\n",
    "            stride = 1\n",
    "        else:\n",
    "            raise ValueError('Output and input channel does not match in residual blocks!!!')\n",
    "\n",
    "    # The first conv layer of the first residual block does not need to be normalized and relu-ed.\n",
    "    with tf.variable_scope('conv1_in_block'):\n",
    "        if first_block:\n",
    "            conv1, conv2, conv3 = conv_layer(input1, [3, 3, input_channel, output_channel], stride, is_training)\n",
    "        else:\n",
    "            conv1, conv2, conv3 = bn_relu_conv_layer(input1, input2, input3, [3, 3, input_channel, output_channel], stride, is_training)\n",
    "\n",
    "    with tf.variable_scope('conv2_in_block'):\n",
    "        conv1, conv2, conv3 = bn_relu_conv_layer(conv1, conv2, conv3, [3, 3, output_channel, output_channel], 1, is_training)\n",
    "\n",
    "    # When the channels of input layer and conv2 does not match, we add zero pads to increase the\n",
    "    #  depth of input layers\n",
    "    if increase_dim is True:\n",
    "        if input_channel * wide_scale == output_channel:\n",
    "            if first_block:\n",
    "                np0 = int((output_channel * l1r - input_channel) / 2)\n",
    "                np1 = int((output_channel * l2r - input_channel) / 2)\n",
    "                np2 = int((output_channel * 1 - input_channel) / 2)\n",
    "                padded_input1 = tf.pad(input1, [[0, 0], [0, 0], [0, 0], [np0, np0]])\n",
    "                padded_input2 = tf.pad(input2, [[0, 0], [0, 0], [0, 0], [np1, np1]])\n",
    "                padded_input3 = tf.pad(input3, [[0, 0], [0, 0], [0, 0], [np2, np2]])\n",
    "            else:\n",
    "                np1 = int((output_channel - input_channel) / 2 * l1r)\n",
    "                np2 = int((output_channel - input_channel) / 2 * l2r)\n",
    "                np3 = int((output_channel - input_channel) / 2)\n",
    "                padded_input1 = tf.pad(input1, [[0, 0], [0, 0], [0, 0], [np1, np1]])\n",
    "                padded_input2 = tf.pad(input2, [[0, 0], [0, 0], [0, 0], [np2, np2]])\n",
    "                padded_input3 = tf.pad(input3, [[0, 0], [0, 0], [0, 0], [np3, np3]])\n",
    "        else:\n",
    "            pooled_input1 = tf.nn.avg_pool(input1, ksize=[1, 2, 2, 1],\n",
    "                                          strides=[1, 2, 2, 1], padding='VALID')\n",
    "            padded_input1 = tf.pad(pooled_input1, [[0, 0], [0, 0], [0, 0], [int(input_channel*l1r) // 2,\n",
    "                                                                            int(input_channel*l1r) // 2]])\n",
    "            pooled_input2 = tf.nn.avg_pool(input2, ksize=[1, 2, 2, 1],\n",
    "                                          strides=[1, 2, 2, 1], padding='VALID')\n",
    "            padded_input2 = tf.pad(pooled_input2, [[0, 0], [0, 0], [0, 0], [int(input_channel*l2r) // 2,\n",
    "                                                                            int(input_channel*l2r) // 2]])\n",
    "            pooled_input3 = tf.nn.avg_pool(input3, ksize=[1, 2, 2, 1],\n",
    "                                          strides=[1, 2, 2, 1], padding='VALID')\n",
    "            padded_input3 = tf.pad(pooled_input3, [[0, 0], [0, 0], [0, 0], [input_channel // 2,\n",
    "                                                                            input_channel // 2]])\n",
    "    else:\n",
    "        padded_input1 = input1\n",
    "        padded_input2 = input2\n",
    "        padded_input3 = input3\n",
    "\n",
    "    output1 = conv1 + padded_input1\n",
    "    output2 = conv2 + padded_input2\n",
    "    output3 = conv3 + padded_input3\n",
    "\n",
    "    return output1, output2, output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_tensor_batch, n, wide_scale, is_train, reuse):\n",
    "\n",
    "    layers1 = []\n",
    "    layers2 = []\n",
    "    layers3 = []\n",
    "\n",
    "    with tf.variable_scope('conv0', reuse=reuse):\n",
    "        std = np.sqrt(6. / (3 * 3 * (3 + 16)))\n",
    "        filter0 = tf.get_variable('conv_l1_l2_l3', initializer=tf.random_uniform([3, 3, 3, 16], -std, std, tf.float32, seed=None))\n",
    "        conv0 = tf.nn.conv2d(input_tensor_batch, filter0, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        layers1.append(conv0)\n",
    "        layers2.append(conv0)\n",
    "        layers3.append(conv0)\n",
    "\n",
    "    for i in range(n):\n",
    "        with tf.variable_scope('block1_%d' %i, reuse=reuse):\n",
    "            if i == 0:\n",
    "                l1conv1, l2conv1, l3conv1 = residual_block(layers1[-1], layers2[-1], layers3[-1], 16, wide_scale, is_train, first_block=True)\n",
    "            else:\n",
    "                l1conv1, l2conv1, l3conv1 = residual_block(layers1[-1], layers2[-1], layers3[-1], 16, wide_scale, is_train)\n",
    "        layers1.append(l1conv1)\n",
    "        layers2.append(l2conv1)\n",
    "        layers3.append(l3conv1)\n",
    "\n",
    "    for i in range(n):\n",
    "        with tf.variable_scope('block2_%d' %i, reuse=reuse):\n",
    "            l1conv2, l2conv2, l3conv2 = residual_block(layers1[-1], layers2[-1], layers3[-1], 32, wide_scale, is_train)\n",
    "        layers1.append(l1conv2)\n",
    "        layers2.append(l2conv2)\n",
    "        layers3.append(l3conv2)\n",
    "\n",
    "    for i in range(n):\n",
    "        with tf.variable_scope('block3_%d' %i, reuse=reuse):\n",
    "            l1conv3, l2conv3, l3conv3 = residual_block(layers1[-1], layers2[-1], layers3[-1], 64, wide_scale, is_train)\n",
    "        layers1.append(l1conv3)\n",
    "        layers2.append(l2conv3)\n",
    "        layers3.append(l3conv3)\n",
    "\n",
    "        assert l3conv3.get_shape().as_list()[1:] == [8, 8, 64*wide_scale]\n",
    "\n",
    "\n",
    "    with tf.variable_scope('fc', reuse=reuse):\n",
    "        in_channel1 = layers1[-1].get_shape().as_list()[-1]\n",
    "        in_channel2 = layers2[-1].get_shape().as_list()[-1]\n",
    "        in_channel3 = layers3[-1].get_shape().as_list()[-1]\n",
    "\n",
    "        bn_layer1 = batch_normalization_layer('l1', layers1[-1], in_channel1)\n",
    "        bn_layer2 = batch_normalization_layer('l2', layers2[-1], in_channel2)\n",
    "        bn_layer3 = batch_normalization_layer('l3', layers3[-1], in_channel3)\n",
    "\n",
    "        relu_layer1 = tf.nn.relu(bn_layer1)\n",
    "        relu_layer2 = tf.nn.relu(bn_layer2)\n",
    "        relu_layer3 = tf.nn.relu(bn_layer3)\n",
    "\n",
    "        global_pool1 = tf.reduce_mean(relu_layer1, [1, 2])\n",
    "        global_pool2 = tf.reduce_mean(relu_layer2, [1, 2])\n",
    "        global_pool3 = tf.reduce_mean(relu_layer3, [1, 2])\n",
    "\n",
    "        assert global_pool3.get_shape().as_list()[-1:] == [64*wide_scale]\n",
    "        output1, output2, output3 = output_layer(global_pool1, global_pool2, global_pool3, 100)\n",
    "\n",
    "\n",
    "    return output1, output2, output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
