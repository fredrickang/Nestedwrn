{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/Nestedwrn/hyperparameters.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'logs_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0;32m--> 633\u001b[0;31m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "%run WRN.ipynb\n",
    "%run input_module.ipynb\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print('Start training...')? (<ipython-input-1-74468f6bc107>, line 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-74468f6bc107>\"\u001b[0;36m, line \u001b[0;32m75\u001b[0m\n\u001b[0;31m    print 'Start training...'\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('Start training...')?\n"
     ]
    }
   ],
   "source": [
    "class Train(object):\n",
    "    def __init__(self):\n",
    "        self.placeholders()\n",
    "\n",
    "    def placeholders(self):\n",
    "        self.image_placeholder = tf.placeholder(dtype=tf.float32, shape=[FLAGS.train_batch_size, IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH])\n",
    "\n",
    "        self.label_placeholder = tf.placeholder(dtype=tf.int32, shape=[FLAGS.train_batch_size])\n",
    "\n",
    "        self.vali_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[FLAGS.validation_batch_size, IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH])\n",
    "\n",
    "        self.vali_label_placeholder = tf.placeholder(dtype=tf.int32, shape=[FLAGS.validation_batch_size])\n",
    "\n",
    "        self.lr_placeholder = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "\n",
    "\n",
    "    def build_train_validation_graph(self):\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        validation_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        logits1, logits2, logits3 = inference(self.image_placeholder, FLAGS.res_blocks, FLAGS.wide_factor, True, reuse=False)\n",
    "        vali_logits1, vali_logits2, vali_logits3 = inference(self.vali_image_placeholder, FLAGS.res_blocks, FLAGS.wide_factor, False, reuse=True)\n",
    "\n",
    "        t_vars = tf.trainable_variables()\n",
    "\n",
    "        regu_loss = sum([tf.nn.l2_loss(w) for w in t_vars])\n",
    "\n",
    "        loss1 = self.loss(logits1, self.label_placeholder)\n",
    "        loss2 = self.loss(logits2, self.label_placeholder)\n",
    "        loss3 = self.loss(logits3, self.label_placeholder)\n",
    "\n",
    "        self.full_loss1 = tf.add_n([loss1]) + FLAGS.weight_decay * regu_loss\n",
    "        self.full_loss2 = tf.add_n([loss2]) + FLAGS.weight_decay * regu_loss\n",
    "        self.full_loss3 = tf.add_n([loss3]) + FLAGS.weight_decay * regu_loss\n",
    "\n",
    "        self.total_loss = 0.5*tf.add_n([loss1]) + 0.3*tf.add_n([loss2]) + 0.2*tf.add_n([loss3]) + FLAGS.weight_decay * regu_loss\n",
    "\n",
    "        predictions1 = tf.nn.softmax(logits1)\n",
    "        predictions2 = tf.nn.softmax(logits2)\n",
    "        predictions3 = tf.nn.softmax(logits3)\n",
    "\n",
    "        self.train_top1_error1 = self.top_k_error(predictions1, self.label_placeholder, 1)\n",
    "        self.train_top1_error2 = self.top_k_error(predictions2, self.label_placeholder, 1)\n",
    "        self.train_top1_error3 = self.top_k_error(predictions3, self.label_placeholder, 1)\n",
    "\n",
    "\n",
    "        # Validation loss\n",
    "        self.vali_loss1 = self.loss(vali_logits1, self.vali_label_placeholder)\n",
    "        vali_predictions1 = tf.nn.softmax(vali_logits1)\n",
    "        self.vali_top1_error1 = self.top_k_error(vali_predictions1, self.vali_label_placeholder, 1)\n",
    "\n",
    "        self.vali_loss2 = self.loss(vali_logits2, self.vali_label_placeholder)\n",
    "        vali_predictions2 = tf.nn.softmax(vali_logits2)\n",
    "        self.vali_top1_error2 = self.top_k_error(vali_predictions2, self.vali_label_placeholder, 1)\n",
    "\n",
    "        self.vali_loss3 = self.loss(vali_logits3, self.vali_label_placeholder)\n",
    "        vali_predictions3 = tf.nn.softmax(vali_logits3)\n",
    "        self.vali_top1_error3 = self.top_k_error(vali_predictions3, self.vali_label_placeholder, 1)\n",
    "\n",
    "        self.train_op = self.train_operation(global_step, self.total_loss, self.train_top1_error3, t_vars)\n",
    "\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        def train(self):\n",
    "\n",
    "            all_data, all_labels = prepare_train_data(padding_size=FLAGS.padding_size)\n",
    "            vali_data, vali_labels = read_validation_data()\n",
    "\n",
    "            self.build_train_validation_graph()\n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess = tf.Session()\n",
    "            sess.run(init)\n",
    "\n",
    "            print 'Start training...'\n",
    "            print '-------------------------------------------------------------------------------------------'\n",
    "\n",
    "            for step in xrange(FLAGS.train_steps):\n",
    "\n",
    "                train_batch_data, train_batch_labels = self.generate_augment_train_batch(all_data, all_labels, FLAGS.train_batch_size)\n",
    "                vali_batch_data, vali_batch_labels = self.generate_vali_batch(vali_data, vali_labels, FLAGS.validation_batch_size)\n",
    "\n",
    "                _ = sess.run([self.train_op],\n",
    "                             {self.image_placeholder: train_batch_data,\n",
    "                              self.label_placeholder: train_batch_labels,\n",
    "                              self.vali_image_placeholder: vali_batch_data,\n",
    "                              self.vali_label_placeholder: vali_batch_labels,\n",
    "                              self.lr_placeholder: FLAGS.init_lr})\n",
    "\n",
    "                tr_l1, tr_e1, tr_l2, tr_e2, tr_l3, tr_e3 = sess.run([self.full_loss1, self.train_top1_error1,\n",
    "                                                                     self.full_loss2, self.train_top1_error2,\n",
    "                                                                     self.full_loss3, self.train_top1_error3],\n",
    "                                                                    {self.image_placeholder: train_batch_data,\n",
    "                                                                     self.label_placeholder: train_batch_labels,\n",
    "                                                                     self.vali_image_placeholder: vali_batch_data,\n",
    "                                                                     self.vali_label_placeholder: vali_batch_labels,\n",
    "                                                                     self.lr_placeholder: FLAGS.init_lr})\n",
    "\n",
    "                if step % FLAGS.report_freq == 0 and step > 0:\n",
    "                    val_l1, val_e1, time1 = self.full_validation(loss=self.vali_loss1, top1_error=self.vali_top1_error1,\n",
    "                                                                 vali_data=vali_data, vali_labels=vali_labels,\n",
    "                                                                 session=sess, batch_data=train_batch_data,\n",
    "                                                                 batch_label=train_batch_labels)\n",
    "\n",
    "                    val_l2, val_e2, time2 = self.full_validation(loss=self.vali_loss2, top1_error=self.vali_top1_error2,\n",
    "                                                                 vali_data=vali_data, vali_labels=vali_labels,\n",
    "                                                                 session=sess, batch_data=train_batch_data,\n",
    "                                                                 batch_label=train_batch_labels)\n",
    "\n",
    "                    val_l3, val_e3, time3 = self.full_validation(loss=self.vali_loss3, top1_error=self.vali_top1_error3,\n",
    "                                                                 vali_data=vali_data, vali_labels=vali_labels,\n",
    "                                                                 session=sess, batch_data=train_batch_data,\n",
    "                                                                 batch_label=train_batch_labels)\n",
    "\n",
    "                    print(\n",
    "                        \"epoch %3d: Train loss1 = %.3f, Val loss1 = %.3f, Train acc1 = %.3f, Val acc1 = %.3f (WRN-%d-%d), time = %.3f  \\n\"\n",
    "                        \"           Train loss2 = %.3f, Val loss2 = %.3f, Train acc2 = %.3f, Val acc2 = %.3f (WRN-%d-%d), time = %.3f  \\n\"\n",
    "                        \"           Train loss3 = %.3f, Val loss3 = %.3f, Train acc3 = %.3f, Val acc3 = %.3f (WRN-%d-%d), time = %.3f, cumulative time = %.3f sec\\n\"\n",
    "                        \"-------------------------------------------------------------------------------------------------------------------------------------------\"\n",
    "                        % (step / FLAGS.report_freq,\n",
    "                           tr_l1, val_l1, 1 - tr_e1, 1 - val_e1, FLAGS.res_blocks*6+2, 1,time1,\n",
    "                           tr_l2, val_l2, 1 - tr_e2, 1 - val_e2, FLAGS.res_blocks*6+2, FLAGS.wide_factor/2, time2,\n",
    "                           tr_l3, val_l3, 1 - tr_e3, 1 - val_e3, FLAGS.res_blocks*6+2, FLAGS.wide_factor, time3, time.time() - start_time))\n",
    "\n",
    "\n",
    "                if step == FLAGS.decay_step0 or step == FLAGS.decay_step1:\n",
    "                    FLAGS.init_lr = 0.1 * FLAGS.init_lr\n",
    "                    print 'Learning rate decayed to ', FLAGS.init_lr\n",
    "\n",
    "            # sys.stdout.close()\n",
    "\n",
    "\n",
    "    ## Helper functions\n",
    "    def loss(self, logits, labels):\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "        return cross_entropy_mean\n",
    "\n",
    "    def compute_sr(self, _theta, th):\n",
    "        ## compute sparse ratio\n",
    "        nz_size = all_size = 0\n",
    "        for i in range(len(_theta)):\n",
    "            nz_size += np.sum([np.abs(_theta[i]) > th])\n",
    "            all_size += np.size(_theta[i])\n",
    "        return float(nz_size) / float(all_size)\n",
    "\n",
    "    def top_k_error(self, predictions, labels, k):\n",
    "        batch_size = predictions.get_shape().as_list()[0]\n",
    "        in_top1 = tf.to_float(tf.nn.in_top_k(predictions, labels, k=1))\n",
    "        num_correct = tf.reduce_sum(in_top1)\n",
    "        return (batch_size - num_correct) / float(batch_size)\n",
    "\n",
    "\n",
    "    def generate_vali_batch(self, vali_data, vali_label, vali_batch_size):\n",
    "        offset = np.random.choice(10000 - vali_batch_size, 1)[0]\n",
    "        vali_data_batch = vali_data[offset:offset+vali_batch_size, ...]\n",
    "        vali_label_batch = vali_label[offset:offset+vali_batch_size]\n",
    "        return vali_data_batch, vali_label_batch\n",
    "\n",
    "\n",
    "    def generate_augment_train_batch(self, train_data, train_labels, train_batch_size):\n",
    "        offset = np.random.choice(EPOCH_SIZE - train_batch_size, 1)[0]\n",
    "        batch_data = train_data[offset:offset+train_batch_size, ...]\n",
    "        batch_data = random_crop_and_flip(batch_data, padding_size=FLAGS.padding_size)\n",
    "        batch_data = whitening_image(batch_data)\n",
    "        batch_label = train_labels[offset:offset+FLAGS.train_batch_size]\n",
    "\n",
    "        return batch_data, batch_label\n",
    "\n",
    "\n",
    "    def train_operation(self, global_step, total_loss, top1_error, var_lists):\n",
    "        opt = tf.train.MomentumOptimizer(learning_rate=self.lr_placeholder, momentum=0.9, use_nesterov=True)\n",
    "        train_op = opt.minimize(total_loss, global_step=global_step, var_list=var_lists)\n",
    "        return train_op\n",
    "\n",
    "\n",
    "    def validation_op(self, validation_step, top1_error, loss):\n",
    "        ema = tf.train.ExponentialMovingAverage(0.0, validation_step)\n",
    "        ema2 = tf.train.ExponentialMovingAverage(0.95, validation_step)\n",
    "\n",
    "        val_op = tf.group(validation_step.assign_add(1), ema.apply([top1_error, loss]),\n",
    "                          ema2.apply([top1_error, loss]))\n",
    "        return val_op\n",
    "\n",
    "\n",
    "    def full_validation(self, loss, top1_error, session, vali_data, vali_labels, batch_data, batch_label):\n",
    "        num_batches = 10000 // FLAGS.validation_batch_size\n",
    "        order = np.random.choice(10000, num_batches * FLAGS.validation_batch_size)\n",
    "        vali_data_subset = vali_data[order, ...]\n",
    "        vali_labels_subset = vali_labels[order]\n",
    "\n",
    "        loss_list = []\n",
    "        error_list = []\n",
    "\n",
    "        t = time.time()\n",
    "        for step in range(num_batches):\n",
    "            offset = step * FLAGS.validation_batch_size\n",
    "            feed_dict = {self.image_placeholder: batch_data, self.label_placeholder: batch_label,\n",
    "                self.vali_image_placeholder: vali_data_subset[offset:offset+FLAGS.validation_batch_size, ...],\n",
    "                self.vali_label_placeholder: vali_labels_subset[offset:offset+FLAGS.validation_batch_size],\n",
    "                self.lr_placeholder: FLAGS.init_lr}\n",
    "            loss_value, top1_error_value = session.run([loss, top1_error], feed_dict=feed_dict)\n",
    "            loss_list.append(loss_value)\n",
    "            error_list.append(top1_error_value)\n",
    "        t_val = time.time() - t\n",
    "\n",
    "        return np.mean(loss_list), np.mean(error_list), t_val/num_batches\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
